<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>The road to JBoss EAP 8</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/24/road-jboss-eap-8" /><author><name>James Falkner</name></author><id>a3d6c262-c5b5-4a9d-a479-3e99000ee793</id><updated>2022-06-24T07:00:00Z</updated><published>2022-06-24T07:00:00Z</published><summary type="html">&lt;p&gt;As a leading, open source, &lt;a href="https://jakarta.ee/compatibility/"&gt;Jakarta Enterprise Edition (Jakarta EE)-compatible&lt;/a&gt; application server, &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) has been a trusted workhorse for enterprise &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; workloads for the past decade. This article describes how the Jakarta EE specifications have evolved since the release of the current version, JBoss EAP 7, and what you can look forward to with JBoss EAP 8.&lt;/p&gt; &lt;p&gt;JBoss EAP 7 is optimized for cloud environments, and when deployed with &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;, offers &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, load balancing, elastic scaling, health monitoring, and the ability to deploy to a container directly from the IDE to improve developer productivity and experience.&lt;/p&gt; &lt;h2&gt;Jakarta EE evolution&lt;/h2&gt; &lt;p&gt;Red Hat is one of the founding members of the &lt;a href="https://jakarta.ee/"&gt;Jakarta EE&lt;/a&gt; working group and has been actively involved in innovating and contributing since the earliest days of Java Enterprise Edition (Java EE). After the move of Java EE from Oracle to Jakarta EE at the Eclipse Foundation, &lt;a href="https://jakarta.ee/news/jakarta-ee-8-released/"&gt;Jakarta EE 8 was released&lt;/a&gt; and set the specifications on a much more vendor-neutral footing, a shift championed by Red Hat and other vendors.&lt;/p&gt; &lt;p&gt;The next major release, &lt;a href="https://jakarta.ee/specifications/platform/9/"&gt;Jakarta EE 9&lt;/a&gt;, did not introduce significant new features but instead made the big move of changing the Jakarta API package namespace from &lt;code&gt;javax.*&lt;/code&gt; to &lt;code&gt;jakarta.*&lt;/code&gt;. The name change introduced a major incompatibility with existing programs. Jakarta EE 9 was largely seen as a stepping stone to the next major version, Jakarta EE 10. JBoss EAP does not support this stepping stone because, without compelling new reasons to switch, the adoption of Jakarta EE 9 would have caused unnecessary churn in our customer base and their applications, with no real added value to compensate.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://eclipse-ee4j.github.io/jakartaee-platform/jakartaee10/JakartaEE10#jakarta-ee-10-schedule"&gt;current plan for Jakarta EE 10&lt;/a&gt; calls for a release in Q2 2022 and presents an opportunity to create a significant revision to the platform compared to Jakarta EE 8 (identical to Java EE 8), and Jakarta EE 9 (with the namespace change).&lt;/p&gt; &lt;p&gt;Several specifications under Jakarta EE are getting new features. One of the more prominent is the introduction of a &lt;a href="https://projects.eclipse.org/projects/ee4j.jakartaee-platform/releases/core-profile-10"&gt;Core profile&lt;/a&gt; (complementing the existing Full and Web profiles that have been in Java EE or Jakarta EE for years). The new profile contains a subset of Jakarta EE specifications that are most useful for smaller, focused applications such as microservices, serverless workloads, and natively ahead-of-time compiled apps. The Core profile is a great step forward for the specification and moves Jakarta EE more into the cloud-native and container space.&lt;/p&gt; &lt;h2&gt;What's planned for JBoss EAP 8&lt;/h2&gt; &lt;p&gt;As always, customers should refer to the officially published &lt;a href="https://access.redhat.com/support/policy/updates/jboss_notes"&gt;life cycles of the Runtimes products&lt;/a&gt; (including EAP) to plan upgrades and migrations for their deployed Runtimes. The information in this article is subject to change without notice.&lt;/p&gt; &lt;p&gt;JBoss EAP is based on the upstream &lt;a href="http://wildfly.org"&gt;WildFly&lt;/a&gt; project. That project is &lt;a href="https://www.wildfly.org/news/2022/01/21/WildFly-2022/"&gt;moving to a feature-boxed release&lt;/a&gt; to align more closely with the way Jakarta EE evolves, and the first version of WildFly to support Jakarta EE 10 will be WildFly 27. So keep an eye on WildFly if you want to get a sneak peek of what's to come in JBoss EAP.&lt;/p&gt; &lt;p&gt;The next major EAP release, JBoss EAP 8, is slated for the first half of 2023, with a Beta release targeting the second half of 2022. The Beta release will give customers a chance to test drive the new features but, more importantly, to plan their migration from earlier versions of JBoss EAP. Along with new features will be an update to platform support for newer versions of &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;, Red Hat OpenShift, Windows, databases, and other dependencies. The final list will be determined closer to release.&lt;/p&gt; &lt;p&gt;Support for the existing version, JBoss EAP 7.x, has been extended in order to provide additional time for current customers to plan their migration to JBoss EAP 8. Consult the &lt;a data-saferedirecturl="https://www.google.com/url?q=https://access.redhat.com/support/policy/updates/jboss_notes%23p_eap&amp;amp;source=gmail&amp;amp;ust=1655388538009000&amp;amp;usg=AOvVaw393mBgGjQ2XOIfUo9dz0ue" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_eap" target="_blank"&gt;Product Support and Update Policy page&lt;/a&gt; for updated support dates for JBoss EAP. &lt;/p&gt; &lt;h2&gt;Product life cycle&lt;/h2&gt; &lt;p&gt;Similar to the current version of JBoss EAP, customers will continue to enjoy long-term support for JBoss EAP 8, corresponding to Red Hat Application Services' &lt;a href="https://access.redhat.com/support/policy/updates/jboss_notes#duration"&gt;Long-life Product Life Cycle&lt;/a&gt;. Coupled with industry-leading &lt;a href="https://www.redhat.com/en/services/support"&gt;24x7 support&lt;/a&gt; and multi-year update and maintenance policies, this commitment gives customers peace of mind that Red Hat can support their most important applications for years to come.&lt;/p&gt; &lt;h2&gt;Migrating to JBoss EAP 8&lt;/h2&gt; &lt;p&gt;EAP 7 continues to be supported based on its &lt;a href="https://access.redhat.com/support/policy/updates/jboss_notes#p_eap"&gt;support life cycle&lt;/a&gt;. Customers should begin planning for migration to EAP 8 to continue to enjoy support beyond the EAP 7 life cycle. As we get closer to EAP 8 Beta later this year, more migration details will be available. But generally, migrations consist of two main areas:  servers and applications.&lt;/p&gt; &lt;h3&gt;Server migration&lt;/h3&gt; &lt;p&gt;This process includes the migration of JBoss EAP configuration files such as &lt;code&gt;standalone.xml&lt;/code&gt;. The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html-single/migration_guide/index#migration_tool_server_migration_tool"&gt;JBoss Server Migration Tool&lt;/a&gt; is the preferred method to update your server configuration to include the new features and settings in new JBoss EAP releases while keeping your existing configuration. The JBoss Server Migration Tool reads your existing JBoss EAP server configuration files and adds configurations for any new subsystems, updates the existing subsystem configurations with new features, and removes any obsolete or "pruned" subsystem configurations.&lt;/p&gt; &lt;p&gt;A preview of this tool will be included as part of the JBoss EAP 8 Beta release. To understand how it works in detail, consult the existing EAP 7 documentation on this migration tool (which will be updated for EAP 8).&lt;/p&gt; &lt;h3&gt;Application migration&lt;/h3&gt; &lt;p&gt;The main migration topic here is the change of Jakarta API namespaces from &lt;code&gt;javax.*&lt;/code&gt; to &lt;code&gt;jakarta.*&lt;/code&gt;. In many cases, this change requires just a simple substitution (and the migration toolkit has new support for things such as &lt;a href="https://docs.openrewrite.org/"&gt;OpenRewrite&lt;/a&gt; recipes to automate this), but not in all cases.&lt;/p&gt; &lt;p&gt;To assist in migrating applications, customers can use the &lt;a href="https://developers.redhat.com/products/mta/overview"&gt;migration toolkit for applications&lt;/a&gt;. This toolkit is an extensible and customizable rule-based set of tools that helps simplify the migration of Java applications. It analyzes the APIs, technologies, and architectures used by the applications you plan to migrate and provides detailed migration reports for each application. This tool will support migrations to JBoss EAP 8 Beta (for testing purposes only) and JBoss EAP 8 GA once it is released. More details on exactly how to migrate to EAP 8 will be available closer to the EAP 8 Beta release.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/topics/application-modernization"&gt;Application migration and modernization&lt;/a&gt; is a large topic by itself. Red Hat continues to innovate here as well, with new projects such as &lt;a href="https://www.redhat.com/architect/tackle-application-modernization"&gt;Tackle&lt;/a&gt; (part of the &lt;a href="https://www.konveyor.io/"&gt;Konveyor&lt;/a&gt; community), which eases the migration and modernization of applications as they transition to the cloud, containers, and Kubernetes.&lt;/p&gt; &lt;p&gt;Keep watching this publication site, as well as the official new &lt;a href="https://twitter.com/redhatjava"&gt;@RedHatJava Twitter stream&lt;/a&gt;, for more details as we approach the release of JBoss EAP 8.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/24/road-jboss-eap-8" title="The road to JBoss EAP 8"&gt;The road to JBoss EAP 8&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>James Falkner</dc:creator><dc:date>2022-06-24T07:00:00Z</dc:date></entry><entry><title type="html">Keycloak 18.0.2 released</title><link rel="alternate" href="https://www.keycloak.org/2022/06/keycloak-1802-released" /><author><name /></author><id>https://www.keycloak.org/2022/06/keycloak-1802-released</id><updated>2022-06-24T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 17.0 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES BUGS * New admin console inaccessible keycloak admin/ui UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title type="html">Profiling to improve DMN file&amp;#8217;s loading time</title><link rel="alternate" href="https://blog.kie.org/2022/06/profiling-to-improve-dmn-files-loading-time.html" /><author><name>Daniel José dos Santos</name></author><id>https://blog.kie.org/2022/06/profiling-to-improve-dmn-files-loading-time.html</id><updated>2022-06-23T19:45:54Z</updated><content type="html">One of my passions is to run profiling tools to find places in the code where performance improvements can be made. Sometimes, we are strongly convinced to know where bottlenecks are, and then, after running a profiler, find out that something completely different is making everything slow. This is one of those cases. In this article, I’ll show you how to profile a GWT application, the DMN Editor. WHAT IS PROFILING? In a nutshell: it is to run a tool against your running application that takes “snapshots” of what is being processed in terms of CPU, memory, I/O, and so on and at the end compare all the “snapshots” to see what it is being shown most of the time in those “snapshots”. FREE PRO-TIP RUNNING ANY PROFILING TOOL Sometimes the entire process that you want to analyze takes too much time, like minutes or even hours. You don’t need to run the profile for all that time otherwise you’ll produce an amount of data so big that analyzing that will be another problem. Of course, if the process takes, for example, 3 minutes but the first minute is slow because of one cause and the remaining minutes are slow because of another thing, a good strategy is to run the profile more times choosing the specific frame time that you want to analyze. WHAT TOOL DO I USE? For this scenario, which is to profile a GWT application, I use the Firefox Profiling Tool available in the Firefox Developer edition. Keep in mind that the tool choice depends on your programming language, where are you running your application (desktop, web), and so on THE REAL CASE In this scenario, we have a very large DMN model that was taking almost 4 minutes to load. And after being loaded, it still doesn’t seem that it was loaded like there is something that is still running in the background causing browser slowness and sometimes making it crash. To start I decided to analyze only the first 30 seconds to see if I got any clue there. RUNNING THE PROFILING TOOL All set, it’s time to run the DMN Editor for the first time! In the Firefox Developer, I hit F12 to show the developer tools. Then I go to the “Performance” tab. Finally, I started the loading of the DMN model which was taking too much time to load. I clicked on “Start recording” and waited a few seconds…. Clicked on “Capture record”. Firefox then loaded my data to profiler.firefox.com, and show me the results with the hot path expanded: Everything is clear now! The method _Kv_g being called by JcG_g called by sjm_g was my bottleneck! Wait… wait… what??? PROFILING GWT A part of our code is written in GWT. For those not aware: the code is written in Java and then “transpiled” to JavaScript. In other words, the code written in Java is “translated” to pure JavaScript code. But obfuscated (not readable by human beings). To debug we use a source map which is handled by the browser. Source map, as its names suggest, it is a “map” that tells the debugger what/where is the real code of the running code. So the method _Kv_g that we saw before is shown in the debugger like executeMyHappyMethod(). Or something like that. But unfortunately, it seems it doesn’t work for profiling. All we saw is the obfuscated code. But don’t panic. You just need to rebuild your application by changing the GWT transpiler from its default “generate obfuscated code” to “generate pretty code”. There are many ways to do that, it depends on the structure of your project but at the end of the day you just need to pass the parameter “-style PRETTY” to the GWT transpiler. In the case of our project, I had to change the pom.xml files to something like that: &lt;plugin&gt;   &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;   &lt;artifactId&gt;gwt-maven-plugin&lt;/artifactId&gt;   &lt;configuration&gt;   &lt;style&gt;PRETTY&lt;/style&gt; (…) the rest of the settings Now everything is ready for a proper profiling phase. In the next post of this series, I finally show how we reduced the loading time of a file from around 3:53 minutes to 33 seconds with a simple change! Stay tuned! The post appeared first on .</content><dc:creator>Daniel José dos Santos</dc:creator></entry><entry><title>Multi-cloud storage strategies for SaaS applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/23/multi-cloud-storage-strategies-saas-applications" /><author><name>Michael Hrivnak</name></author><id>954895a5-82f1-45f3-8b0f-fedb7d86c6a4</id><updated>2022-06-23T07:00:00Z</updated><published>2022-06-23T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;SaaS architecture checklist&lt;/a&gt; is a series of articles that cover the software and deployment considerations for &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-saas"&gt;Software as a Service (SaaS)&lt;/a&gt; applications. In this fourth article in the series, you'll learn about software SaaS providers can use to simplify their storage architecture while still accommodating a multi-cloud strategy.&lt;/p&gt; &lt;h2&gt;Bring your service to your customer's platforms&lt;/h2&gt; &lt;p&gt;A multi-cloud strategy is critical if you want to enable your SaaS application to run in the same physical location that your end customers use. Running on the same network is essential when a SaaS processes a large amount of end customer data or integrates closely with end customer systems. This keeps latency low and avoids expensive charges for inter-region or inter-cloud bandwidth. Since it is unlikely that a SaaS provider’s entire target market uses the same cloud provider, they must operate their service on multiple cloud platforms. In other words, you must bring your service to your customers.&lt;/p&gt; &lt;p&gt;The first step is to choose a portable Kubernetes distribution, such as &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, to normalize the operational burden across clouds. Your goal should be to avoid having to do unique development, configuration, lifecycle management, change management, and API integration for each deployment platform (see Figure 1). But certain capabilities remain cloud-specific, and &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;persistent storage&lt;/a&gt; is one that almost every SaaS provider needs to use. If your SaaS application stores persistent data, you can still be left dealing with feature differences among the storage offerings of various clouds, such as dynamic provisioning, shared access, snapshots, security controls, and which of the three types of storage—block, filesystem, or object—are offered. Accommodating those differences can lead to costly efforts by development, operations, and support teams.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Multi-cloud%20storage-image1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Multi-cloud%20storage-image1.png?itok=rJQtPUV9" width="600" height="599" alt="Red Hat OpenShift infrastructure experience" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Red Hat OpenShift provides a consistent infrastructure experience across cloud environments. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/topics/data-storage/software-defined-storage"&gt;Software-defined storage&lt;/a&gt; (SDS) is a common architectural pattern that provides a consistent feature set across public and private cloud environments. SDS is sometimes referred to as &lt;em&gt;hyperconverged infrastructure&lt;/em&gt; (HCI), where raw storage devices, compute resources, software-defined storage solutions, and workloads are all part of the same Kubernetes cluster. One SDS system can run in many cloud environments as long as block devices are available to individual hosts within the cluster.&lt;/p&gt; &lt;p&gt;Software-defined storage decouples hardware storage devices from the logical storage mechanisms accessed by software. For example, three servers could have one or more physical storage devices available to the SDS system. The SDS system would be installed across the three servers and configured to use the physical devices as one pool (see Figure 2). With a pool of raw storage, you can configure the system to aggregate and expose the raw storage as a logical block, filesystem, or object storage over the network. You can also configure the system for redundancy, maintaining a set number of copies of data across multiple underlying hosts.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/software-defined-storage_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/software-defined-storage_0.png?itok=r90t4g4-" width="600" height="450" alt="software-defined storage diagram" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: A software-defined storage system utilizes raw storage devices from multiple nodes in an OpenShift cluster, presenting them as a highly-available pool of storage. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;A team of software solutions&lt;/h2&gt; &lt;p&gt;These three technologies can be combined to form a complete software-defined storage solution:&lt;/p&gt; &lt;h3&gt;Ceph&lt;/h3&gt; &lt;p&gt;&lt;a href="https://ceph.com/"&gt;Ceph&lt;/a&gt; is a popular open source software-defined storage platform (and number one for OpenStack) that can turn commodity block devices across multiple hosts into a high-performance, fault-tolerant storage cluster. It exposes object, filesystem, and block storage used by applications running in Kubernetes. Ceph has excellent management capabilities, including snapshots, cloning, and automated rebalancing and recovery. Ceph has become a popular choice for software-defined storage due to its rich feature set enabling a variety of storage use cases and the flexibility to run almost anywhere.&lt;/p&gt; &lt;h3&gt;Rook&lt;/h3&gt; &lt;p&gt;&lt;a href="https://rook.io/"&gt;Rook&lt;/a&gt; is a &lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Kubernetes Operator&lt;/a&gt; that offers a Kubernetes-native method for deploying and managing Ceph. According to Rook's website, it "turns distributed storage systems into self-managing, self-scaling, self-healing storage services. It automates the tasks of a storage administrator: deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management." Rook has become a leading option because it offers enterprise-grade storage within a Kubernetes cluster through the HCI approach.&lt;/p&gt; &lt;h3&gt;Noobaa&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.noobaa.io/"&gt;Noobaa&lt;/a&gt; adds advanced object storage capabilities, including deduplication, compression, and encryption at rest. Mirroring and replication policies distribute data across multiple backing storage services, even in different cloud environments.&lt;/p&gt; &lt;p&gt;Bringing these capabilities into an OpenShift cluster is easy with &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation"&gt;Red Hat OpenShift Data Foundation&lt;/a&gt;, because it combines Ceph, Rook, and Noobaa as one optional operator. Following the HCI pattern with OpenShift and OpenShift Data Foundation enables you to operate your SaaS application across multiple public and private clouds with just one storage platform and one Kubernetes platform. Operations teams prefer having only one software lifecycle to track, a consistent set of APIs and capabilities, a limited surface area of domain expertise to maintain, and a single point of support. Development teams save time by focusing on one storage and compute platform for testing and validation.&lt;/p&gt; &lt;h2&gt;Learn more about Kubernetes storage&lt;/h2&gt; &lt;p&gt;To learn more about Kubernetes storage solutions, a great resource is &lt;a href="https://www.redhat.com/en/engage/kubernetes-containers-storage-s-201911201051"&gt;Storage Patterns for Kubernetes For Dummies, Red Hat Special Edition&lt;/a&gt;. Chapter two covers specific details about this topic. For further explanation of storage primitives and concepts on OpenShift, refer to the &lt;a href="https://docs.openshift.com/container-platform/4.10/storage/index.html"&gt;OpenShift Container Platform Storage Overview&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/23/multi-cloud-storage-strategies-saas-applications" title="Multi-cloud storage strategies for SaaS applications"&gt;Multi-cloud storage strategies for SaaS applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Hrivnak</dc:creator><dc:date>2022-06-23T07:00:00Z</dc:date></entry><entry><title>Measuring BPF performance: Tips, tricks, and best practices</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/22/measuring-bpf-performance-tips-tricks-and-best-practices" /><author><name>Dmitrii Dolgov, Giles Hutton</name></author><id>ac707552-31bd-4ac8-8dc0-505801fafd8b</id><updated>2022-06-22T07:00:00Z</updated><published>2022-06-22T07:00:00Z</published><summary type="html">&lt;p&gt;Measuring stuff is harder than you may think. Even in math, there is such a thing as non-measurable sets and &lt;a href="https://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox"&gt;Banach-Tarski paradox&lt;/a&gt;, which shows that a three-dimensional ball of radius 1 can be dissected into 5 parts, which can be magically reassembled to form two balls of radius 1. Now imagine what kind of mess we've got in software engineering.&lt;/p&gt; &lt;p&gt;This may sound somewhat philosophical, so let's get down to earth and talk about BPF programs. BPF, which originally stood for &lt;em&gt;Berkeley Packet Filter,&lt;/em&gt; is a general-purpose execution engine that can be used for a variety of purposes, including networking, observability, and security. What we'll be discussing in this article was initially called &lt;em&gt;extended BPF&lt;/em&gt; (eBPF) to differentiate from the classic BPF, but now this technology is &lt;a href="https://www.brendangregg.com/systems-performance-2nd-edition-book.html"&gt;often referred to as just BPF&lt;/a&gt;, and that's what we'll call it here.&lt;/p&gt; &lt;p&gt;Having full visibility throughout the systems we build is a well established best practice. Usually, one knows which metrics to collect, and how and what to profile or instrument to understand why the system exhibits certain levels of performance. But all of this becomes more challenging as soon as the BPF layer is included. Despite years of development, it is still a black box in many ways.&lt;/p&gt; &lt;p&gt;But do not despair: this article will show, with some amount of creativity, that one can reveal what is going on under the hood. The article will begin with a userspace service that uses BPF programs attached to various syscalls to inspect system activity. It does the job, but what we really want is to understand the overhead of the BPF programs themselves. How do you look inside? That's the problem that we will ultimately tackle.&lt;/p&gt; &lt;h2&gt;Anatomy of a system call&lt;/h2&gt; &lt;p&gt;There is a tradition of capturing explanations of system calls (syscalls) under the title "&lt;a href="http://lwn.net/Articles/604287/"&gt;anatomy of a system call&lt;/a&gt;," which we diligently follow. Since we acquire most of the information from syscalls, the first step is understanding how they work.&lt;/p&gt; &lt;p&gt;Each architecture application binary interface (ABI) has requirements influencing how system calls work. Let’s limit our discussion to x86_64. The standard ABI for how x86_64 is to put the system call number into the &lt;code&gt;RAX&lt;/code&gt; register and the other parameters into specific registers, then issue the &lt;a href="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-2b-manual.html"&gt;SYSCALL&lt;/a&gt; instruction, goes something like this:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;code&gt;SYSCALL&lt;/code&gt; invokes an OS system call handler at privilege level 0. It does so by loading RIP from the IA32_LSTAR MSR after saving the address of the instruction following syscall into RCX. The &lt;code&gt;WRMSR&lt;/code&gt; instruction ensures that the IA32_LSTAR MSR always contains a canonical address.&lt;/p&gt; &lt;p&gt;At system startup, the kernel provides a custom &lt;code&gt;entry_SYSCALL_64&lt;/code&gt; function for handling syscalls and writes the address of this handler function to the &lt;code&gt;MSR_LSTAR&lt;/code&gt; register. The final step is to call the function pointer from the system call table using the number stored in &lt;code&gt;RAX&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now, there could be a set of hooks specified, to be invoked before the execution of the actual syscall. It could be a custom probe registered for the corresponding static tracepoint or dynamic kprobe. We need to use them to instrument our application. Here is where things get tricky.&lt;/p&gt; &lt;h2&gt;Who traces the tracer?&lt;/h2&gt; &lt;p&gt;The main problem with measuring performance is measuring the right thing. In the case of measuring the performance of a program responsible for tracing another, this can be quite challenging. Consider &lt;code&gt;sys_enter&lt;/code&gt; and &lt;code&gt;sys_exit&lt;/code&gt; tracepoints that fire before or after a syscall is executed. We have BPF programs performing some activity when triggering those tracepoints. The exercise is to measure time spent in those programs.&lt;/p&gt; &lt;p&gt;There is some machinery in the kernel itself to collect statistics about BPF program runtime that give us cumulative time and number of runs for a particular program:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ sysctl -w kernel.bpf_stats_enabled=1 $ bpftool prog [...] 379: raw_tracepoint [...] run_time_ns 35875602162 run_cnt 160512637 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Such aggregated numbers are good for sanity checks; but usually, they are not enough.&lt;/p&gt; &lt;p&gt;Another naive approach is to follow the same strategy as instrumenting normal userspace applications. Trace the same points, looking at any difference in performance (i.e., overall syscall latency) between the tracing program running and not running. All things considered, this approach could work, but only if you can guarantee that the performance-measuring probe is executed at the right time: before the start of the tracing program, and after it finishes (see Figure 1).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/syscall_tracing_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/syscall_tracing_1.png?itok=JPSDGPWF" width="243" height="400" alt="Two BPF programs are attached to the sys_enter/sys_exit tracing points of the syscall" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Two BPF programs are attached to the sys_enter/sys_exit tracing points of the syscall. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;But alas, this will never happen because BPF programs are designed to be independent. So it barely makes sense to talk about their ordering.&lt;/p&gt; &lt;p&gt;Given the difficulty of getting the execution timing right, can we measure from a different point? Our first thought was to measure inside the probes themselves instead. That way, we could ignore all the cruft of the kernel running the syscall and focus on our execution. If we can measure that correctly, then that should give us the overhead. Simple, right? In theory, yes. But in practice, it was a bit more complicated.&lt;/p&gt; &lt;p&gt;The main difficulty was getting the information out of the BPF probe or the kernel module. Initially, we tried a quick and dirty approach of writing a message to the trace pipe and then consuming and processing those events to get the data.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; //somewhere inside your BPF prog bpf_trace_printk("Start timestamp: %lld", ts); &lt;/code&gt; &lt;/pre&gt; &lt;pre&gt; &lt;code&gt; $ cat /sys/kernel/debug/tracing/trace_pipe $ bpftool prog tracelog &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This gave us promising initial results. However, it was not practical because it introduced significant measurement overhead.&lt;/p&gt; &lt;p&gt;Another point to measure could be a kernel function on the way to the syscall. We wondered whether there was kernel code around the syscall that we can attach to instead, guaranteeing the desirable probe execution order. By examining stacktraces captured via ftrace, we found that there is a small function that does just that. On amd64, it is called &lt;code&gt;do_syscall_64&lt;/code&gt;. It is slightly higher in the call stack than &lt;code&gt;sys_enter&lt;/code&gt; and &lt;code&gt;sys_exit&lt;/code&gt;, so we can measure latency from the start of &lt;code&gt;do_syscall_64&lt;/code&gt; to the end of the function, which will include any overhead from our probes. Is this ideal, or not?&lt;/p&gt; &lt;h2&gt;Tale of two probes&lt;/h2&gt; &lt;p&gt;One minor inconvenience with &lt;code&gt;do_syscall_64&lt;/code&gt; is that on kernels newer than 5.6, it can’t be used for this measurement because of a notrace marking. In this sense, we end up at the mercy of kernel internals. But there are more annoying problems along the way.&lt;/p&gt; &lt;p&gt;When we first fired up the measurements using kprobe and kretprobe to attach to the start/finish, we noticed that while the kprobe fired consistently, there was only a small number of kretprobe executions. Obviously, this didn’t make any sense because the calls were definitely complete, and there was no problem in userspace indicating that the syscalls were running correctly. Luckily, the &lt;a href="https://docs.kernel.org/trace/kprobes.html#how-does-a-return-probe-work"&gt;kernel documentation&lt;/a&gt; provided some answers.&lt;/p&gt; &lt;p&gt;A kprobe implements a kretprobe on the function entry. That probe stores the return address of the function and overwrites it with a trampoline address. When the function returns, the trampoline calls into the user-provided probe (i.e., our performance measuring BPF program). So why don’t we see all the returns?&lt;/p&gt; &lt;p&gt;While the probed function is executing, an object of type &lt;code&gt;kretprobe_instance&lt;/code&gt; stores its return address. Before calling &lt;code&gt;register_kretprobe()&lt;/code&gt;, the user sets the &lt;code&gt;maxactive&lt;/code&gt; field of the kretprobe struct to specify how many instances of simultaneous probing for the specified function can be probed simultaneously. &lt;code&gt;register_kretprobe()&lt;/code&gt; pre-allocates the indicated number of kretprobe_instance objects.&lt;/p&gt; &lt;p&gt;The problem is that there is a limit to the number of simultaneously stored return addresses. If you're running on a very busy codepath (like on every system call), then you're going to fill up this list very quickly and inevitably drop many events. The first experiments utilized &lt;code&gt;bpftrace&lt;/code&gt;, which uses the perf API and does not support specifying &lt;code&gt;maxactive&lt;/code&gt;. However, configuration is possible in a custom bcc script. In our case, we had to set it to a large value to capture all the events that occurred.&lt;/p&gt; &lt;p&gt;If you are curious, there is an alternative approach. The functions &lt;code&gt;kfunc&lt;/code&gt; and &lt;code&gt;kretfunc&lt;/code&gt; in &lt;code&gt;bpftrace&lt;/code&gt; use the fentry/fexit mechanism and BPF trampolines to do the same process, and they are not affected by kprobe machinery with &lt;code&gt;maxactive&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;When you find a good solution, look for a better one&lt;/h2&gt; &lt;p&gt;This solution is good, but not perfect. It is a reasonable way to collect the measurements. It covers not only BPF programs but also the underlying syscall. This brings a lot of variance and annoying noise into the captured data. Is there a way to make the measurements narrower?&lt;/p&gt; &lt;p&gt;The answer: BPF Type Format (BTF).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/counter-small_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/counter-small_1.png?itok=1w58ZEG-" width="600" height="248" alt="Number of abbreviations is going down" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Shows the number of abbreviations going down. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: With the introduction of BTF, there is one fewer three-letter abbreviation available.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;When BTF information is available for the BPF program, the kernel JIT compiler emits instructions for providing fentry/fexit (seems like jumps &lt;code&gt;0xE9&lt;/code&gt; ). We could use those to attach two more monitoring BPF programs to the start/finish of our target BPF program and collect various events in between. In this way, one measures only a particular BPF program without including anything else. In fact, &lt;code&gt;bpftool prog profile&lt;/code&gt; and new &lt;code&gt;perf stat -b&lt;/code&gt; work in the same way.&lt;/p&gt; &lt;h2&gt;Profiling BPF programs&lt;/h2&gt; &lt;p&gt;So far, this discussion has revolved around getting counters between two points. But we can profile our BPF programs to understand what is happening at the instruction level.&lt;/p&gt; &lt;p&gt;We can profile a BPF program as part of any other kernel activity, but there seems to be no way to profile only one program. However, there is one possible approach to acquiring the tag of the BPF program: record samples of kernel activity, then annotate only those parts.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ bpftool prog id 5 5: tracepoint name &lt;tp_name&gt; tag 19c24c00f06f9dce gpl $ perf record ... $ perf annotate -l -P bpf_prog19c24c00f06f9dce [...] // bpf_prog_19c24c00f06f9dce[48] 13.56 : 48:callq 0xffffffffc5dfe8cc 0.00 : 4d:mov -0x8(%rbp),%edi 0.00 : 50:and $0x2,%rdi 0.00 : 54:test %rdi,%rdi [...] &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The resulting annotated instructions match exactly to the JITed dump of the BPF program from bpftool.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Hopefully, the information described here helps to shed some light on the darkest corners of your system. There is still much more to learn and develop, but a journey of a thousand miles begins with a first step.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/22/measuring-bpf-performance-tips-tricks-and-best-practices" title="Measuring BPF performance: Tips, tricks, and best practices"&gt;Measuring BPF performance: Tips, tricks, and best practices&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Dmitrii Dolgov, Giles Hutton</dc:creator><dc:date>2022-06-22T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.10.0.Final released - Preliminary work on Loom's virtual threads and various refinements all over the place</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-10-0-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-10-0-final-released/</id><updated>2022-06-22T00:00:00Z</updated><content type="html">New month, new Quarkus feature release, you know the drill: Quarkus 2.10.0.Final has landed. This version is a mix of exploratory work and refinements on existing extensions: Preliminary work on Loom’s virtual threads GraphQL non-blocking support Kubernetes service binding support for Reactive SQL Clients CacheKeyGenerator for cache extension And much...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Distributed tracing with OpenTelemetry, Knative, and Quarkus</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/21/distributed-tracing-opentelemetry-knative-and-quarkus" /><author><name>Daniel Oh</name></author><id>159af59e-7aa1-4648-9cee-cc085b280c9a</id><updated>2022-06-21T07:00:00Z</updated><published>2022-06-21T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt; project addresses the needs of modern, &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; environments running &lt;a href="https://developers.redhat.com/topics?source=sso"&gt;microservices&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and cloud systems. This article contains a video showing how to integrate OpenTelemetry into a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application using the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; framework.&lt;/p&gt; &lt;h2&gt;Scaling metrics collection&lt;/h2&gt; &lt;p&gt;Today's scalable application platforms leave site reliability engineers (SREs) with the significant burden of collecting telemetry data ranging from a single cluster to multiple clouds. Contrast this challenge with traditional environments from years ago—you could probably get by with dashboards then, perhaps from third-party application performance monitoring (APM) tools.&lt;/p&gt; &lt;p&gt;Application topologies are getting more complex due to distributed, cloud-native microservices. Infrastructure environments also range from &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; containers to &lt;a href="https://developers.redhat.com/topics/iot"&gt;Internet of Things (IoT)&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge devices&lt;/a&gt;, public clouds, and multiple Kubernetes installations. This diversity is also a new challenge for SRE teams and developers who want to trace application chains.&lt;/p&gt; &lt;h2&gt;Benefits of OpenTelemetry&lt;/h2&gt; &lt;p&gt;To solve the challenge, the &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt; project was created through a merger of the &lt;a href="https://opentracing.io/"&gt;OpenTracing&lt;/a&gt; and &lt;a href="https://opencensus.io/"&gt;OpenCensus&lt;/a&gt; projects. The key benefits of OpenTelemtry include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create and ingest telemetry data from application services to IoT edge devices.&lt;/li&gt; &lt;li&gt;&lt;span&gt;Increase observability to monitor metrics, logging, and event tracing by &lt;/span&gt;&lt;span&gt;a consistent collection mechanism. &lt;/span&gt;&lt;/li&gt; &lt;li&gt;Integrate with polyglot frameworks such as Quarkus, &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;.NET Core&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once the OpenTelemetry collector or client gathers telemetry data such as metrics, logs, and traces from your applications or app servers, you can send the metrics to a back-end server such as &lt;a href="https://www.jaegertracing.io"&gt;Jaeger&lt;/a&gt; to analyze applications' performance and behavior.&lt;/p&gt; &lt;p&gt;To take advantage of this powerful new platform, you have to install and configure the OpenTelemetry server itself, then change existing application projects to incorporate OpenTelemetry's capabilities. All this reworking has to take place on cloud-native microservices and serverless functions on Kubernetes.&lt;/p&gt; &lt;h2&gt;Distributed tracing with Quarkus&lt;/h2&gt; &lt;p&gt;The video in this article shows how Quarkus integrates the OpenTelemetry features for distributed tracing with serverless functions on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The OpenShift cluster enables developers to deploy cloud-native microservices as serverless functions using &lt;a href="https://knative.dev/docs/serving/"&gt;Knative serving&lt;/a&gt;. Find more information about Knative &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The steps shown in the video include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On your local system: Implement multiple reactive, RESTful applications.&lt;/li&gt; &lt;li&gt;On your local system: Collect local telemetry data by the OpenTelemetry collector in Quarkus Dev mode.&lt;/li&gt; &lt;li&gt;On your local system: Visualize the telemetry data through the Jaeger console locally.&lt;/li&gt; &lt;li&gt;On the remote Kubernetes system: Install the OpenTelemetry and Jaeger Operators using operators on Red Hat OpenShift.&lt;/li&gt; &lt;li&gt;On the remote Kubernetes system: Deploy the reactive applications as serverless functions using Quarkus extensions and Knative on Red Hat OpenShift.&lt;/li&gt; &lt;li&gt;On the remote Kubernetes system: Verify the telemetry data that OpenTelemetry collects instantly when the serverless functions get started and send the data to the Jaeger server.&lt;/li&gt; &lt;/ul&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can find the complete code for the example in my &lt;a href="https://github.com/danieloh30/quarkus-opentelemetry"&gt;GitHub repository&lt;/a&gt; and replicate the tutorial in your development environment.&lt;/p&gt; &lt;h2&gt;Continue your cloud-native journey with Quarkus&lt;/h2&gt; &lt;p&gt;Here are more resources to get familiar with the ways Quarkus enables you to reduce your development time locally and jumpstart your development in Kubernetes without a lot of installation tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/28/build-your-first-java-serverless-function-using-quarkus-quick-start"&gt;Build your first Java serverless function using a Quarkus quick start&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;Learn Quarkus faster with quick starts in the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/02/11/enhancing-the-development-loop-with-quarkus-remote-development"&gt;Enhancing the development loop with Quarkus remote development&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/21/distributed-tracing-opentelemetry-knative-and-quarkus" title="Distributed tracing with OpenTelemetry, Knative, and Quarkus"&gt;Distributed tracing with OpenTelemetry, Knative, and Quarkus&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Oh</dc:creator><dc:date>2022-06-21T07:00:00Z</dc:date></entry><entry><title type="html">Using Qute with templates from a database</title><link rel="alternate" href="https://quarkus.io/blog/qute-templates-from-db/" /><author><name>Gwenneg Lepage</name></author><id>https://quarkus.io/blog/qute-templates-from-db/</id><updated>2022-06-21T00:00:00Z</updated><content type="html">Introduction I’m part of a Red Hat team that created a multitenant notifications service which sends the notifications from many Red Hat Hybrid Cloud Console apps (the tenants). Our service can be used to send several kinds of notifications, including emails. Each tenant can create as many email templates as...</content><dc:creator>Gwenneg Lepage</dc:creator></entry><entry><title>Install Cryostat with the new Helm chart</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/20/install-cryostat-new-helm-chart" /><author><name>Elliott Baron</name></author><id>c0362cda-ca71-4d5c-bee6-3b2e280eb079</id><updated>2022-06-20T07:00:00Z</updated><published>2022-06-20T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cryostat.io/"&gt;Cryostat&lt;/a&gt; is a tool for managing &lt;a href="https://developers.redhat.com/blog/2020/08/25/get-started-with-jdk-flight-recorder-in-openjdk-8u"&gt;JDK Flight Recorder&lt;/a&gt; data on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. &lt;a href="articles/2022/06/08/9-awesome-updates-cryostat-21"&gt;Cryostat 2.1&lt;/a&gt; is now installable using a Helm chart. While the &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; is our preferred installation method for production environments, the Cryostat &lt;a href="https://developers.redhat.com/articles/2022/04/14/generate-helm-charts-your-java-application-using-jkube-part-1"&gt;Helm chart&lt;/a&gt; is a better choice for demo purposes. The Helm chart has a flexible design and requires few permissions to allow many users as needed.&lt;/p&gt; &lt;h2&gt;How to install the Cryostat Helm chart&lt;/h2&gt; &lt;p&gt;The Cryostat Helm chart is included by default in the &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; versions 4.8 and up. From the OpenShift developer console, click &lt;strong&gt;Add+&lt;/strong&gt;, and then select &lt;strong&gt;Helm Chart&lt;/strong&gt; from the developer catalog card. Search for &lt;strong&gt;Cryostat&lt;/strong&gt; and choose &lt;strong&gt;Install Helm Chart&lt;/strong&gt; (see Figure 1).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cryostat-helm-1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cryostat-helm-1.png?itok=UA-wz6Og" width="600" height="328" alt="Installing the Cryostat Helm Chart from the Developer Catalog" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Installing the Cryostat Helm Chart from the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;To install the Cryostat Helm chart on OpenShift 4.6-4.7, add the OpenShift Helm chart repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm repo add openshift-helm-charts https://charts.openshift.io/ $ helm install &lt;release_name&gt; cryostat&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Kubernetes users can install the community-supported version of the Helm chart from our GitHub repository:&lt;/p&gt; &lt;pre&gt; $ helm install &lt;release_name&gt; \ https://github.com/cryostatio/cryostat-helm/releases/download/v0.1.3/cryostat-0.1.3.tgz&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Installing the Cryostat Helm chart creates the following objects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A deployment containing Cryostat, Grafana, and a data source for Grafana.&lt;/li&gt; &lt;li&gt;Services for Cryostat and Grafana.&lt;/li&gt; &lt;li&gt;A service account, role, and role binding for Cryostat to use for discovering your applications.&lt;/li&gt; &lt;li&gt;By default on OpenShift, routes to expose the Cryostat and Grafana services outside of the clusters.&lt;/li&gt; &lt;li&gt;If enabled, ingresses to expose the Cryostat and Grafana services outside of the clusters.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There are a variety of configuration options to customize these objects. Our chart README contains a complete list of these options for &lt;a href="https://github.com/openshift-helm-charts/charts/blob/main/charts/redhat/redhat/cryostat/0.1.1/src/README.md"&gt;OpenShift&lt;/a&gt; and &lt;a href="https://github.com/cryostatio/cryostat-helm/blob/cryostat-v2.1/cryostat/README.md"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Once the Helm chart is installed, it will prompt you to run additional commands which tell Cryostat’s components how to communicate with each other. The &lt;code&gt;ClusterIP&lt;/code&gt; service type with no route or ingress will illustrate how to use port forwarding with &lt;code&gt;kubectl&lt;/code&gt; or &lt;code&gt;oc&lt;/code&gt;. Run these commands in a terminal on your local machine with &lt;code&gt;kubectl&lt;/code&gt; or &lt;code&gt;oc&lt;/code&gt; logged into the cluster where you installed the Helm chart. Figure 2 illustrates an example of these instructions.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cryostat-helm-2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cryostat-helm-2.png?itok=hzrg70W3" width="600" height="328" alt="Post-install steps for the Cryostat Helm Chart shown in the Topology View" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Post-install steps for the Cryostat Helm Chart shown in the topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;After running the commands listed in the &lt;strong&gt;Release notes&lt;/strong&gt; tab, the Cryostat deployment will roll out a new replica set. Once the new replica set is fully rolled out and the Cryostat deployment is available, click the &lt;strong&gt;Open URL&lt;/strong&gt; (&lt;img alt="Open URL" data-entity-type="file" data-entity-uuid="eb086c9e-381c-4c92-acf6-81ad5965bc04" src="https://developers.redhat.com/sites/default/files/inline-images/openurl.png" width="14" height="14" loading="lazy" /&gt;) button on the Cryostat deployment. This will bring you to the Cryostat web application served by the deployment.&lt;/p&gt; &lt;h2&gt;A comparison of the Cryostat Operator and the Cryostat Helm chart&lt;/h2&gt; &lt;p&gt;In the future, we plan to close the feature gap between the Cryostat Operator and Helm chart. This table compares their features:&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="NaN"&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td width="NaN"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;&lt;strong&gt;Cryostat Helm chart&lt;/strong&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;&lt;strong&gt;Cryostat Operator&lt;/strong&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Grafana integration&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;span&gt;Access Cryostat using services&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;td class="text-align-center" width="NaN"&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;span&gt;Access Cryostat using ingresses or routes&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;End-to-end encryption&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;/td&gt; &lt;td class="text-align-center" width="NaN"&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/sso-all-cryostats-new-openshift-login-flow"&gt;Authentication&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td class="text-align-center" width="NaN"&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Persistent storage&lt;/p&gt; &lt;/td&gt; &lt;td class="text-align-center" width="NaN"&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/eat-fewer-resources-cryostat-21-sidecar-reports"&gt;Report generator microservice&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td class="text-align-center" width="NaN"&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p class="text-align-center"&gt;X&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Now there are two easy methods to install Cryostat into your cluster: the Cryostat Helm chart and the Cryostat Operator. You can obtain both of these for OpenShift from the &lt;a href="https://docs.openshift.com/container-platform/4.10/applications/creating_applications/odc-creating-applications-using-developer-perspective.html"&gt;Developer Catalog&lt;/a&gt; and &lt;a href="https://docs.openshift.com/container-platform/4.10/operators/user/olm-installing-operators-in-namespace.html"&gt;OperatorHub&lt;/a&gt;, respectively.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/20/install-cryostat-new-helm-chart" title="Install Cryostat with the new Helm chart"&gt;Install Cryostat with the new Helm chart&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Elliott Baron</dc:creator><dc:date>2022-06-20T07:00:00Z</dc:date></entry><entry><title>Learn about OpenShift command-line tools</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/16/learn-about-openshift-command-line-tools" /><author><name>Varsha Sharma</name></author><id>3b088a43-eb42-4132-b7cc-09be54bdceeb</id><updated>2022-06-16T07:00:00Z</updated><published>2022-06-16T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; simplifies application deployment, the management and monitoring of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; clusters, and other developer tasks. The OpenShift tools, both command-line interface (CLI) and graphical user interface (GUI), cover many crucial deployment tasks. This article focuses on the &lt;code&gt;oc&lt;/code&gt; and &lt;code&gt;odo&lt;/code&gt; CLI commands, but touches on the GUIs as well.&lt;/p&gt; &lt;h2&gt;The oc client&lt;/h2&gt; &lt;p&gt;The OpenShift &lt;code&gt;oc&lt;/code&gt; command helps you develop, build, deploy, and run applications on an OpenShift or Kubernetes cluster. The command includes an &lt;code&gt;adm&lt;/code&gt; subcommand for administering the cluster.&lt;/p&gt; &lt;p&gt;&lt;code&gt;oc&lt;/code&gt; can be downloaded from the &lt;a href="https://developers.redhat.com/openshift/command-line-tools"&gt;help menu of your OpenShift web interface&lt;/a&gt;, from &lt;a href="https://github.com/openshift/origin/releases"&gt;OpenShift's GitHub releases page&lt;/a&gt;, or from the Red Hat web site after you create a Red Hat account. There are &lt;a href="https://access.redhat.com/downloads/content/290/ver=4.10/rhel---8/4.10.10/x86_64/product-software"&gt;binaries for many operating systems&lt;/a&gt;, including Linux, Microsoft Windows, and macOS.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;--help&lt;/code&gt; option (flag) offers an overview of the &lt;code&gt;oc&lt;/code&gt; command. This option can be also used with subcommands, which can help you build the right command in your early stages of use. For example, the following command displays options for the &lt;code&gt;whoami&lt;/code&gt; subcommand:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc whoami --help Show information about the current session The default options for this command will return the currently authenticated user name or an empty string. Other flags support returning the currently used token or the user context. Usage: oc whoami [flags] Examples: # Display the currently authenticated user oc whoami Options: --show-console=false: If true, print the current server's web console URL -c, --show-context=false: Print the current user context name --show-server=false: If true, print the current server's REST API URL -t, --show-token=false: Print the token the current session is using. This will return an error if you are using a different form of authentication. Use "oc options" for a list of global command-line options (applies to all commands). $ oc whoami --show-console https://console-openshift-console.apps.varsha.lab.upshift.rdu2.redhat.com&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Several more examples of &lt;code&gt;oc&lt;/code&gt; commands appear in the following subsections.&lt;/p&gt; &lt;h3&gt;View the console&lt;/h3&gt; &lt;p&gt;You can pull up the OpenShift console from the command line by entering:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;oc whoami --show-console&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Log in to an OpenShift cluster&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;login&lt;/code&gt; subcommand connects you to one of your clusters:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc login -u &lt;username&gt; -p &lt;password&gt; &lt;servername&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc login -u kubeadmin -p asdfghjkliuytr https://upi-o.varsha.lab.upshift.rdu2.redhat.com:6443&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can use an access token to log in. Obtain a token by visiting:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;https://oauth-openshift.apps.varsha.lab.upshift.rdu2.redhat.com/oauth/token/request&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A login using a token looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc login --token=sha256~vNVfZ0VuFK7SkveM_nGRFTS7nrfawCQnQ9FEPNScv-0 --server=https://api.varsha.lab.upshift.rdu2.redhat.com:6443&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Switch to a specific project&lt;/h3&gt; &lt;p&gt;Specify the name of your project with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc project &lt;project_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;List existing projects&lt;/h3&gt; &lt;p&gt;You can view the projects in your account like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc project list $ oc projects&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Display the current project&lt;/h3&gt; &lt;p&gt;To see which project you are logged into, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc project&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Run applications&lt;/h3&gt; &lt;p&gt;Here are some useful commands to use with applications.&lt;/p&gt; &lt;p&gt;Execute a command on a particular pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc exec &lt;mypod&gt; -- &lt;command_to_execute_in_pod&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Start a remote shell so you can enter a series of commands on a pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc rsh &lt;pod_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Debug cluster activities on a pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc debug &lt;pod_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Frequently used commands&lt;/h3&gt; &lt;p&gt;Some more common activities are:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# See an overview of the current project oc status # -f, --follow=false: Specify if the logs should be streamed. oc logs -f &lt;pod_name&gt; # list all pods resource in ps output format oc get pods oc get po # list all resources in a namespace oc get all # Show details of a specific resource or group of resources. Print a detailed description of the selected resources, including related resources such as events or controllers. oc describe pod &lt;pod_name&gt; # List all services together in ps output format oc get services # To display output of yaml file of a resource oc get &lt;resource&gt; &lt;resource_name&gt; -o yaml # Delete all resources of the application oc delete all -l app=&lt;app_name&gt; # Delete all pods oc delete pod &lt;pod_name&gt; &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Scaling applications&lt;/h3&gt; &lt;p&gt;Autoscaling increases or decreases the number of pods in the cluster to meet demand for its service. OpenShift makes autoscaling easy. For instance, the following command creates an autoscaler for the specified deployment, replica set, stateful set, or replication controller:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc scale dc &lt;resource_name&gt; --replicas=&lt;count&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also specify parameters for autoscaling. For instance, the following command allows OpenShift to scale the application between 2 and 4 pods with a target CPU utilization of 80%:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc autoscale deployment &lt;resource_name&gt; --min=2 --max=4 --cpu-percent=80%&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Deploy a ConfigMap&lt;/h3&gt; &lt;p&gt;A ConfigMap is a key-value listing used in many aspects of configuration. Normally, the ConfigMap is stored as YAML in a file and pushed to the cluster, as shown in the following command. The &lt;code&gt;--from-file&lt;/code&gt; option can be specified multiple times in a single command to create the ConfigMap from a combination of files:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create configmap &lt;name&gt; --from-file=&lt;file_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Mount the ConfigMap into the pods via the deployment controller or deployment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc set volume deployment/&lt;resource_name&gt; --add --name=&lt;config_map_volume&gt; -m &lt;mount_path&gt; -t configmap --configmap-name=&lt;name&gt; --default-mode='0777'&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Deploying a secret&lt;/h3&gt; &lt;p&gt;A secret is a key-value listing protected by encryption, and is used for sensitive data such as passwords. The following command creates a secret and mounts it as a volume to a deployment configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic &lt;secret_name&gt; --from-literal=username=myuser --from-file=&lt;file_name&gt; $ oc set volume dc/&lt;resource_name&gt; --add --name=&lt;secret_volume&gt; -m &lt;mount_path&gt; -t secret --secret-name=&lt;secret_name&gt; --default-mode='0755'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The odo client&lt;/h2&gt; &lt;p&gt;&lt;a href="https://odo.dev/docs/getting-started/features"&gt;odo&lt;/a&gt; (an abbreviation of &lt;em&gt;OpenShift do&lt;/em&gt;) is a developer-focused command that runs OpenShift applications in an expedited and automated manner. &lt;code&gt;odo&lt;/code&gt; reduces the developer's cognitive load. This command is for application developers who wish to deploy their applications to Kubernetes easily without spending a lot of time learning the &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; and Kubernetes procedures for deploying enterprise infrastructure. In contrast, &lt;code&gt;oc&lt;/code&gt; focuses on operations and requires a deeper understanding of Kubernetes and OpenShift concepts.&lt;/p&gt; &lt;p&gt;&lt;code&gt;odo&lt;/code&gt; can be installed from its &lt;a href="//odo.dev/docs/getting-started/installation"&gt;web page&lt;/a&gt; and has a &lt;a href="https://github.com/redhat-developer/odo"&gt;GitHub repository&lt;/a&gt; with documentation.&lt;/p&gt; &lt;h3&gt;Getting started with odo&lt;/h3&gt; &lt;p&gt;Here are a few commands to try in order to get familiar with &lt;code&gt;odo&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You can create an application as follows. The final &lt;code&gt;push&lt;/code&gt; subcommand builds the application, pushes it to your cluster, and deploys it on OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/openshift/nodejs-ex $ cd nodejs-ex $ odo create nodejs $ odo push&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;watch&lt;/code&gt; subcommand tells &lt;code&gt;odo&lt;/code&gt; to automatically redeploy your application after edits:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ odo watch&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can edit your code in real time and watch as &lt;code&gt;odo&lt;/code&gt; automatically deploys your application.&lt;/p&gt; &lt;p&gt;You can create a URL where clients can send HTTP requests to your application as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ odo url create myurl $ odo push&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following commands give you more information about the cluster, such as logs or what components you've deployed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ odo describe $ odo list $ odo log&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Visual Studio Code OpenShift Connector for Red Hat OpenShift: An odo GUI&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-openshift-connector&amp;ssr=false#overview"&gt;OpenShift Connector extension&lt;/a&gt;, also known as the Visual Studio Code OpenShift Connector for Red Hat OpenShift, is a plug-in for the &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;VSCode IDE&lt;/a&gt; that interacts with Red Hat OpenShift clusters. Basically, the OpenShift Connector extension provides a GUI for &lt;code&gt;odo&lt;/code&gt; as well as &lt;code&gt;oc&lt;/code&gt;, combining groups of related commands into compact units. Developers can use the interface to accomplish complex tasks with minimal OpenShift experience.&lt;/p&gt; &lt;p&gt;To start with the connector, choose a predefined template, such as a Project, Application, or Service. The graphical interface can then build the resource and deploy it to your cluster as an OpenShift component. The OpenShift Connector can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create applications&lt;/li&gt; &lt;li&gt;Build applications&lt;/li&gt; &lt;li&gt;Debug applications&lt;/li&gt; &lt;li&gt;Deploy the applications directly to an OpenShift cluster&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The latest version of OpenShift Connector is available in the &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-openshift-connector"&gt;VS Code Marketplace&lt;/a&gt;. There are several options available for different environments, so you can use the interface directly on a laptop or &lt;a href="http://developers.redhat.com/products/codeready-containers"&gt;install it into a local OpenShift cluster&lt;/a&gt;. (There are limitations on Apple M1 hardware.)&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article should get you started on OpenShift using the command line or a GUI. You will grow in sophistication over time, and use the tools for such advanced tasks as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gathering information from a cluster for debugging issues&lt;/li&gt; &lt;li&gt;Working directly with project source code&lt;/li&gt; &lt;li&gt;Managing project content when the web console is not accessible&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/16/learn-about-openshift-command-line-tools" title="Learn about OpenShift command-line tools"&gt;Learn about OpenShift command-line tools&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Varsha Sharma</dc:creator><dc:date>2022-06-16T07:00:00Z</dc:date></entry></feed>
